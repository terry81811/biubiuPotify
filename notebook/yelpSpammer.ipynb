{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spammer Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "With the popularization of the computer and the developing of the internet, ranking websites such as Yelp, Imdb, IGN become popular in the society. More and more people rely on review system when making their options. In order to build up a robust review system, it is very important to find out which reviews are reliable and fair. \n",
    "This project is going to find out what kind of features in either reviews or users might end up with a spam or spammer. We choose Yelp restaurant reviews in Pittsburgh area as our datasets. Apply different machine learning methods to get the hypothesis features that might led to a spam or spammer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "In order to get all the reviews and relevant features from Yelp in a single restaurant, we use python beautifulsoup4 package to acquire these data. First, we focus on every single reviews and figure out what kinds of features is most likely to be a spam feature. However, even if we can sort out the spam precisely and delete it, these spams might come out again and again in our review system. Therefore, Next step, we focus on finding the specific features in users that a spammers may have. We can track down those users and delete their membership. The final step is to find out the possible restaurant that is hiring a spammer. We can warn those restaurant or cancel their post if need it. With all the features we got in our models, we can apply these to other review system and make a more robust review system without spams and spammers.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oscar Wang\\Anaconda3\\envs\\PDS\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named requests",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3b89ca08356a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0myelpAPI\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named requests"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.tokenize.punkt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "import io, time, json\n",
    "import sys\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from yelpAPI import *\n",
    "\n",
    "from collections import OrderedDict\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "# import yelp client library\n",
    "from yelp.client import Client\n",
    "from yelp.oauth1_authenticator import Oauth1Authenticator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acqusition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature Collection\n",
    "\n",
    "Basically, we are extensively collecting everything from Yelp.com. Since we are not only interested in review text but also meta data of users and restaurants, we can categorize our data into the following three categories.\n",
    "\n",
    "Some of the features can be extracted directly from Yelp pages but others have to be aggregated or even calculated using graph relations between restaurants and users. \n",
    "* Review level:\n",
    "    - Review time\n",
    "    - Review length\n",
    "    - Review rating\n",
    "    - Percentage of capital words\n",
    "    - Subjectivity/Objectivity of review\n",
    "    - Polarity of review  \n",
    "    - Language model (unigram, bigram)\n",
    "* User level:\n",
    "    - Number of reviews\n",
    "    - Number of friends\n",
    "    - Number of photos\n",
    "    - Has profile photo or not\n",
    "    - Yelp Since\n",
    "    - Rating Distribution\n",
    "    - Average Rating\n",
    "    - Ratio of Positive Rating\n",
    "    - Ratio of Negative Rating\n",
    "    - Rating Deviation \n",
    "    - Active duration\n",
    "    - Entropy of review ratings\n",
    "    - Average Review Length\n",
    "    - Average Percentage of capital words\n",
    "    - Average Subjectivity/Objectivity of review\n",
    "    - Average Polarity of review  \n",
    "* Restaurant level: \n",
    "    - Number of reviews\n",
    "    - Number of recommended reviews\n",
    "    - Number of non recommended reviews\n",
    "    - Average Rating\n",
    "    - Ratio of Positive Rating\n",
    "    - Ratio of Negative Rating\n",
    "    - Max review in a day\n",
    "    - Entropy of review ratings\n",
    "    - Average Review Length\n",
    "    - Average Percentage of capital words\n",
    "    - Average Subjectivity/Objectivity of review\n",
    "    - Average Polarity of review \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler\n",
    "\n",
    "We starts with 1000 restaurants in Pittsburgh and obtained their reviews and a list of users who reviewed this restaurant. These gave us about 36000 users and 86000 reviews. We then use another crawler to crawl user level data. One problem we are facing is that when we were collecting user level data, we found that yelp only provide review information about recommend or not recommend for restaurants, which in our case, is the label on restaurant pages. (On user pages, yelp shows all reviews with indicating whether they are recommended or not.)\n",
    "To overcome this problem and get correct information at user level, we have to crawl one more level deeper. We get another restaurant list from those 36000 users and we crawl every restaurant they visited and get the correct label of those reviews we encountered in user pages.\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Restaurant review crawler:\n",
    "# parse recommended page:\n",
    "\n",
    "def parse_page(html):\n",
    "\n",
    "\tsoup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\tl = list()\n",
    "\n",
    "\ttry:\n",
    "\t\trecommend_review_container = soup.find('div', class_=\"review-list\")\n",
    "\t\treviews = recommend_review_container.find_all('div', class_=\"review review--with-sidebar\")\n",
    "\n",
    "\t\tfor r in reviews:\n",
    "\t\t\ttry:\n",
    "\t\t\t\treview_id = r.get('data-review-id')\n",
    "\n",
    "\t\t\t\tuser_photo = r.find('a', class_=\"user-photo js-analytics-click\")\n",
    "\t\t\t\tuser_id = r.get('data-signup-object').split(\":\")[1]\n",
    "\t\t\t\trating = r.find('div', class_=\"rating-very-large\").get('title').split(\" \")[0]\n",
    "\t\t\t\tdate = re.sub(r\"\\s+\", '', r.find('span', class_=\"rating-qualifier\").get_text())\n",
    "\t\t\t\ttext = r.find('div', class_=\"review-content\").find('p').get_text()\n",
    "\n",
    "\t\t\t\tl.append({\n",
    "\t\t\t\t\t'review_id': review_id,\n",
    "\t\t\t\t\t'user_id': user_id,\n",
    "\t\t\t\t\t'rating': float(rating),\n",
    "\t\t\t\t\t'date': date,\n",
    "\t\t\t\t\t'text': text,\t\t   \n",
    "\t\t\t\t\t})\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint \">>> EXCEPTION: a review can not be extract. Ignore\"\n",
    "\t\t\t\ttraceback.print_exc()\n",
    "\t\t\t\tpass\n",
    "\texcept:\n",
    "\t\tprint \">>> EXCEPTION: reviews can not be extract. Ignore\"\n",
    "\t\ttraceback.print_exc()\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "\t# find next page\n",
    "\tnext = soup.find_all('a', class_=\"u-decoration-none next pagination-links_anchor\")\n",
    "\n",
    "\tif len(next) != 0:\n",
    "\t\tnextLink = next[0].get('href')\n",
    "\telse:\n",
    "\t\tnextLink = \"\"\n",
    "\n",
    "\treturn (l, nextLink)\n",
    "\n",
    "def extract_reviews(url):\n",
    "\t\n",
    "\tpl = list()\n",
    "\tpUrl = url\n",
    "\n",
    "\twhile pUrl != \"\":\n",
    "\t\t# sleep(random.random() * 2)\n",
    "\n",
    "\t\thtml = retrieve_html(pUrl)[1]\n",
    "\t\tpRes = parse_page(html)\n",
    "\n",
    "\t\tpl.extend(pRes[0])\n",
    "\t\tpUrl = pRes[1]\n",
    "\n",
    "\treturn pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-2-38f465492763>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-38f465492763>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    print \">>> EXCEPTION: a review can not be extract. Ignore\"\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "# Restaurant review crawler:\n",
    "# parse recommended page:\n",
    "\n",
    "def parse_page_not_recommend(html):\n",
    "\n",
    "\tsoup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\t# find reviews\n",
    "\tl = list()\n",
    "\n",
    "\ttry:\n",
    "\t\tnot_recommended_reviews = soup.find('div', class_='not-recommended-reviews')\n",
    "\t\treviews = not_recommended_reviews.find_all('div', class_=\"review review--with-sidebar\")\n",
    "\n",
    "\t\tfor r in reviews:\n",
    "\t\t\ttry:\n",
    "\t\t\t\treview_id = r.get('data-review-id')\n",
    "\t\t\t\tuser_id = r.get('data-signup-object').split(\":\")[1]\n",
    "\t\t\t\trating = r.find('div', class_=\"rating-very-large\").get('title').split(\" \")[0]\n",
    "\t\t\t\tdate = re.sub(r\"\\s+\", '', r.find('span', class_=\"rating-qualifier\").get_text())\n",
    "\t\t\t\ttext = r.find('div', class_=\"review-content\").find('p').get_text()\n",
    "\n",
    "\t\t\t\tl.append({\n",
    "\t\t\t\t\t'review_id': review_id,\n",
    "\t\t\t\t\t'user_id': user_id,\n",
    "\t\t\t\t\t'rating': float(rating),\n",
    "\t\t\t\t\t'date': date,\n",
    "\t\t\t\t\t'text': text,\t\t   \n",
    "\t\t\t\t\t})\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint \">>> EXCEPTION: a review can not be extract. Ignore\"\n",
    "\t\t\t\tpass\n",
    "\texcept:\n",
    "\t\tprint \">>> EXCEPTION: reviews can not be extract. Ignore\"\n",
    "\t\ttraceback.print_exc()\n",
    "\t\tpass\n",
    "\n",
    "\t# find next page\n",
    "\tpagination = soup.find('div', 'pagination-links arrange_unit')\n",
    "\tnextLink = \"\"\n",
    "\tif pagination is not None:\n",
    "\t\tnext = pagination.find_all('a', class_=\"u-decoration-none next pagination-links_anchor\")\n",
    "\t\tif len(next) != 0:\n",
    "\t\t\tnextLink = next[0].get('href')\n",
    "\n",
    "\treturn (l, nextLink)\n",
    "\n",
    "def extract_unrecommend_reviews(url):\n",
    "\tnl = list()\n",
    "\tnUrl = url\n",
    "\n",
    "\twhile nUrl != \"\":\n",
    "\t\t# sleep(random.random() * 2)\n",
    "\n",
    "\t\thtml = retrieve_html('https://www.yelp.com' + nUrl)[1]\n",
    "\t\tnRes = parse_page_not_recommend(html)\n",
    "\n",
    "\t\tnl.extend(nRes[0])\n",
    "\t\tnUrl = nRes[1]\n",
    "\n",
    "\treturn nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main function to load reviews of 1000 restaurants in Pittsburgh\n",
    "\n",
    "f_reviews_content = io.open('outputData/reviews_content_' + subset + '.tsv', 'w', encoding='utf8')\n",
    "f_userIdMapping = io.open('outputData/user_id_mapping_' + subset + '.tsv', 'w', encoding='utf8')\n",
    "f_metaData = io.open('outputData/metaData_' + subset + '.tsv', 'w', encoding='utf8')\n",
    "\n",
    "userIdDict = {}\n",
    "\n",
    "# read 1000 restaurants in Pittsburgh\n",
    "\n",
    "f = open('outputData/businessesIdMapping.tsv', 'r')\n",
    "businessLines = [line.rstrip('\\n\\r') for line in f]\n",
    "\n",
    "cnt = 1\n",
    "\n",
    "for businessLine in businessLines[int(subset): int(subset) + 100]:\n",
    "\n",
    "\tbusinessId = businessLine.split('\\t')[0]\n",
    "\n",
    "\tprint \">>> \" + businessId + \" extracting recommended reviews from: \" + businessLine.split('\\t')[1]\n",
    "\turl = 'https://www.yelp.com/biz/' + businessLine.split('\\t')[1]\n",
    "\treviews = extract_reviews(url)\n",
    "\tprint \">>> extrated \" + str(len(reviews)) + \" from \" + businessLine.split('\\t')[1]\n",
    "\n",
    "\n",
    "\tfor review in reviews:\n",
    "\n",
    "\t\t# added user to user set\n",
    "\t\tif review['user_id'] not in userIdDict:\n",
    "\t\t\tuserIdDict[review['user_id']] = {'pos': 0, 'neg': 1}\n",
    "\t\telse:\n",
    "\t\t\tuserIdDict[review['user_id']]['neg'] += 1\n",
    "\n",
    "\t\tf_reviews_content.write(str(cnt) + \"\\t\" + review['review_id'] + \"\\t\" + review['text'] + \"\\n\")\n",
    "\t\tf_metaData.write(str(cnt) + \"\\t\" + review['user_id'] + \"\\t\" + businessId + \"\\t\" + str(review['rating']) + \"\\t\" + \"1\\t\" + review['date'] + \"\\n\")\n",
    "\t\tcnt += 1\n",
    "\n",
    "\n",
    "\tprint \">>> \" + businessId + \"  extracting non-recommended reviews from: \" + businessLine.split('\\t')[1]\n",
    "\turl = '/not_recommended_reviews/' + businessLine.split('\\t')[1]\n",
    "\treviews = extract_unrecommend_reviews(url)\n",
    "\tprint \">>> extrated \" + str(len(reviews)) + \" from \" + businessLine.split('\\t')[1]\n",
    "\n",
    "\tfor review in reviews:\n",
    "\n",
    "\t\t# added user to user set\n",
    "\t\tif review['user_id'] not in userIdDict:\n",
    "\t\t\tuserIdDict[review['user_id']] = {'pos': 1, 'neg': 0}\n",
    "\t\telse:\n",
    "\t\t\tuserIdDict[review['user_id']]['pos'] += 1\n",
    "\n",
    "\t\tf_reviews_content.write(str(cnt) + \"\\t\" + review['review_id'] + \"\\t\" + review['text'] + \"\\n\")\n",
    "\t\tf_metaData.write(str(cnt) + \"\\t\" + review['user_id'] + \"\\t\" + businessId + \"\\t\" + str(review['rating']) + \"\\t\" + \"-1\\t\" + review['date'] + \"\\n\")\n",
    "\n",
    "\t\tcnt += 1\n",
    "\n",
    "print \"... finished crawling, extracted \" + str(cnt-1) + \" reviews in total\"\n",
    "\n",
    "cnt = 1\n",
    "for userId, count in userIdDict.iteritems():\n",
    "\tf_userIdMapping.write(str(cnt) + \"\\t\" + userId + \"\\t\" + str(count['neg']) + \"\\t\" + str(count['pos']) + \"\\n\")\n",
    "\tcnt+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user page crawler\n",
    "\n",
    "def parseUserReviews(html):\n",
    "\tsoup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\t# find reviews\n",
    "\tl = list()\n",
    "\n",
    "\ttry:\n",
    "\t\t# not_recommended_reviews = soup.find('div', class_='not-recommended-reviews')\n",
    "\t\t# businesses = soup.find_all('a', class_=\"biz-name js-analytics-click\")\n",
    "\n",
    "\t\treviewContainer = soup.find('div', class_='user-details_reviews')\n",
    "\t\treviews = reviewContainer.find_all('div', class_=\"review\")\n",
    "\n",
    "\t\tfor r in reviews:\n",
    "\t\t\ttry:\n",
    "\t\t\t\treviewId = r.get('data-review-id')\n",
    "\t\t\t\tbizId = r.find('a', class_=\"biz-name js-analytics-click\").get('href').split(\"/\")[2]\n",
    "\t\t\t\treviewContent = r.find('div', class_=\"review-content\").find('p').get_text()\n",
    "\n",
    "\t\t\t\tl.append({\n",
    "\t\t\t\t\t'reviewId': reviewId,\n",
    "\t\t\t\t\t'bizId': bizId,\n",
    "\t\t\t\t\t'reviewContent': reviewContent,\t\t   \n",
    "\t\t\t\t\t})\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint \">>> EXCEPTION: a review can not be extract. Ignore\"\n",
    "\t\t\t\tpass\n",
    "\n",
    "\t\t# find next page\n",
    "\t\tpagination = soup.find('div', 'pagination-links arrange_unit')\n",
    "\t\tnextLink = \"\"\n",
    "\t\tif pagination is not None:\n",
    "\t\t\tnext = pagination.find_all('a', class_=\"u-decoration-none next pagination-links_anchor\")\n",
    "\t\t\tif len(next) != 0:\n",
    "\t\t\t\tnextLink = next[0].get('href')\n",
    "\n",
    "\t\treturn (l, nextLink)\n",
    "\n",
    "\texcept:\n",
    "\t\tprint \">>> EXCEPTION: reviews can not be extract. Ignore\"\n",
    "\t\ttraceback.print_exc()\n",
    "\t\treturn (l, \"\")\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "def getReviewFromUserPage(userId):\n",
    "\n",
    "\treviewList = list()\n",
    "\turl = 'https://www.yelp.com/user_details_reviews_self?userid=' + userId + '&rec_pagestart=0'\n",
    "\n",
    "\twhile url != \"\":\n",
    "\t\thtml = retrieve_html(url)[1]\n",
    "\t\tres = parseUserReviews(html)\n",
    "\n",
    "\t\treviewList.extend(res[0])\n",
    "\t\turl = res[1]\n",
    "\n",
    "\treturn reviewList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# crawl user data\n",
    "# return rows of (userId, neg, pos)\n",
    "def getSuspiciousUsers(inputFile, outputFile):\n",
    "\tprint \">>> in getSuspiciousUsers\"\n",
    "\n",
    "\tf_in = open('outputData_Final/' + inputFile + '.tsv', 'r')\n",
    "\tuserParts = [line.rstrip('\\n\\r').split(\"\\t\") for line in f_in]\n",
    "\n",
    "\tf_out = open('outputData_Final/' + outputFile + '.tsv', 'w')\n",
    "\tfor userPart in userParts:\n",
    "\t\tif int(userPart[2]) + int(userPart[3]) > 1 and int(userPart[3]) > 0:\n",
    "\t\t\tf_out.write(\"\\t\".join(userPart) + \"\\n\")\n",
    "\n",
    "# return rows of (reviewId, userId, businessId)\n",
    "def crawlUserReviews(inputFile, outputFile1, outputFile2, start):\n",
    "\tprint \">>> in crawlUserReviews\"\n",
    "\n",
    "\tf_in = open('outputData_Final/' + inputFile + '.tsv', 'r')\n",
    "\tuserParts = [line.rstrip('\\n\\r').split(\"\\t\") for line in f_in]\n",
    "\n",
    "\tprint \">>> preparing to read \" + str(len(userParts)) + \" user reviews\"\n",
    "\n",
    "\t# sleep for 2 minute between every 100 users\n",
    "\tfor offsetIndex in range(start, len(userParts) / 100 + 1):\n",
    "\t\toffset = offsetIndex * 100\n",
    "\t\t# print offsetIndex\n",
    "\t\tf_out_meta = io.open('outputData_Final/suspiciousReviewerReviews/' + outputFile1 + \"_\" + str(offset) + '.tsv', 'w', encoding='utf8')\n",
    "\t\tf_out_content = io.open('outputData_Final/suspiciousReviewerReviews/' + outputFile2 + \"_\" + str(offset) + '.tsv', 'w', encoding='utf8')\n",
    "\n",
    "\t\tcnt = 1\n",
    "\t\tfor part in userParts[offset: offset + 100]:\n",
    "\t\t\treviews = getReviewFromUserPage(part[1])\n",
    "\t\t\tprint \">>> \" + part[0] + \"  extracted \" + str(len(reviews)) + \" from user: \" + part[1]\n",
    "\n",
    "\t\t\tfor review in reviews:\n",
    "\n",
    "\t\t\t\tf_out_meta.write(str(cnt) + \"\\t\" + review['reviewId'] + \"\\t\" + part[1] + \"\\t\" + review['bizId'] + \"\\n\")\n",
    "\t\t\t\tf_out_content.write(str(cnt) + \"\\t\" + review['reviewId'] + \"\\t\" + review['reviewContent'] + \"\\n\")\n",
    "\n",
    "\t\t\t\tcnt += 1\n",
    "\n",
    "\t\tf_out_meta.close()\n",
    "\t\tf_out_content.close()\n",
    "\n",
    "\t\tprint \">>> file: \" + str(offset) + \" is written. sleep for 2 minutes...\"\n",
    "\t\tsleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-5-a24ef95af51a>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-a24ef95af51a>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    print \">>> EXCEPTION: could not find notRecommend Count. return 0\"\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "# helper functions\n",
    "def getBizFromUserPage(userId):\n",
    "\n",
    "\tbizList = list()\n",
    "\turl = 'https://www.yelp.com/user_details_reviews_self?userid=' + userId + '&rec_pagestart=0'\n",
    "\n",
    "\twhile url != \"\":\n",
    "\t\thtml = retrieve_html(url)[1]\n",
    "\t\tres = parseUserPage(html)\n",
    "\n",
    "\t\tbizList.extend(res[0])\n",
    "\t\turl = res[1]\n",
    "\n",
    "\treturn bizList\n",
    "\n",
    "\n",
    "def getMetaDataFromBiz(bizId):\n",
    "\n",
    "\turl = \"https://www.yelp.com/biz/\" + bizId\n",
    "\thtml = retrieve_html(url)[1]\n",
    "\tsoup = BeautifulSoup(html, 'html.parser')\n",
    "\tinfoContainer = soup.find('div', class_='biz-main-info embossed-text-white')\n",
    "\tstars = infoContainer.find('div', class_='rating-very-large').get('title').split(\" \")[0]\n",
    "\trecommendCountText = infoContainer.find('span', 'review-count rating-qualifier').get_text()\n",
    "\trecommendCount = ' '.join(recommendCountText.split()).split(\" \")[0]\n",
    "\n",
    "\ttry:\n",
    "\t\tnotRecommendedContainer = soup.find('div', class_= 'not-recommended ysection')\n",
    "\t\tnotRecommendCountText = notRecommendedContainer.find('a', class_='subtle-text').get_text()\n",
    "\t\tnotRecommendCount = ' '.join(notRecommendCountText.split()).split(\" \")[0]\n",
    "\n",
    "\texcept:\n",
    "\t\tprint \">>> EXCEPTION: could not find notRecommend Count. return 0\"\n",
    "\t\tnotRecommendCount = '0'\n",
    "\n",
    "\turl = \"https://www.yelp.com/not_recommended_reviews/\" + bizId\n",
    "\n",
    "\treturn [stars, recommendCount, notRecommendCount]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Spam \n",
    "\n",
    "Normally, spam review usually has some special features that we can use for detecting. As mentioned above, our ultimate goal is to determine whether restaurants have people writing spam review for them or not. Hence, the first step is to determine if a single review content is fake. In order to do that, we first use text classification that we learned in class to do simple spam modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Labels\n",
    "\n",
    "Before learning the classifier, we need the true labels of our training review data. Here we use the labels provided by Yelp website, which indicates that a review is “recommended” or “not recommended”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "\n",
    "To build our text classifier, we process the review text, use bag of words and tfidf techniques to the training data. As mentioned in the lecture slide, bag of words work pretty effectively but throwing information. So we also use N-gram to improve our classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Features\n",
    "\n",
    "### Subjectivity and Objectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('inputData/metaData_all.tsv', 'r')\n",
    "metaDataLines = [line.rstrip('\\n\\r') for line in f]\n",
    "\n",
    "f = codecs.open('inputData/reviews_content_all.tsv', 'r', 'utf8')\n",
    "reviewsContentLines = [line.rstrip('\\n\\r') for line in f]\n",
    "\n",
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "\n",
    "\n",
    "train_subj_docs = subj_docs[:80]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
    "\n",
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
    "\n",
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "\n",
    "testing_docs = reviewsContentLines[0: 50]\n",
    "\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)\n",
    "\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = codecs.open('inputData/reviews_content_all.tsv', 'r', 'utf8')\n",
    "reviewsContent = [line.rstrip('\\n\\r').split(\"\\t\")[2] for line in f]\n",
    "f = open('inputData/metaData_all.tsv', 'r')\n",
    "metaDataLines = [line.rstrip('\\n\\r') for line in f]\n",
    "\n",
    "sentences = reviewsContent[0:1000]\n",
    "categories = {\n",
    "\t'1.0': {'cnt': 0, 'neg': 0, 'neu': 0, 'pos': 0},\n",
    "\t'2.0': {'cnt': 0, 'neg': 0, 'neu': 0, 'pos': 0},\n",
    "\t'3.0': {'cnt': 0, 'neg': 0, 'neu': 0, 'pos': 0},\n",
    "\t'4.0': {'cnt': 0, 'neg': 0, 'neu': 0, 'pos': 0},\n",
    "\t'5.0': {'cnt': 0, 'neg': 0, 'neu': 0, 'pos': 0}\n",
    "}\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for i in range(len(sentences)):\n",
    "    print(sentences[i][0:50] + \" ...\")\n",
    "    ss = sid.polarity_scores(sentences[i])\n",
    "    print ss\n",
    "    ratings = metaDataLines[i].split(\"\\t\")[3]\n",
    "    categories[ratings]['cnt'] += 1\n",
    "    categories[ratings]['neg'] += ss['neg']\n",
    "    categories[ratings]['neu'] += ss['neu']\n",
    "    categories[ratings]['pos'] += ss['pos']\n",
    "\n",
    "for k, v in categories.iteritems():\n",
    "\tprint \">>> ratings: \" + k\n",
    "\tprint \"neg: \" + str(float(v['neg'])/v['cnt'])\n",
    "\tprint \"neu: \" + str(float(v['neu'])/v['cnt'])\n",
    "\tprint \"pos: \" + str(float(v['pos'])/v['cnt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spammer Modeling\n",
    "\n",
    "### Review feature model\n",
    "\n",
    "In the review datasets, we have the label of whether it is a recommend review or not. We use that label as our output and implement 5 machine learning model to train.\n",
    "\n",
    "- SVM (Support vector machine)\n",
    "- MLP (Multilayer perceptron)\n",
    "- KNN (K-nearest neighbors algorithm)\n",
    "- Decision Tree\n",
    "- Logistic Regression\n",
    "\n",
    "We split all the datasets into train(80%) and validation(20%), drop the redundant column and train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855908367344\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('review_features_labeled.TSV', delimiter='\\t')\n",
    "train, test = train_test_split(df, test_size = 0.2, stratify = df['label'])\n",
    "X_tr = train.drop(['k', 'biz', 'user', 'date', 'label'], axis=1)\n",
    "y_tr = train['label']\n",
    "X_te = test.drop(['k', 'biz', 'user', 'date', 'label'], axis=1)\n",
    "y_te = test['label']\n",
    "print float(df[df.label == 1].shape[0]) / float(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856022978876\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier()\n",
    "clf.fit(X_tr,y_tr)\n",
    "y_pre = clf.predict(X_te)\n",
    "print np.mean(y_pre==y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.821614505416\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(3)\n",
    "clf.fit(X_tr,y_tr)\n",
    "y_pre = clf.predict(X_te)\n",
    "print np.mean(y_pre==y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856920591227\n"
     ]
    }
   ],
   "source": [
    "#DT\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=5)\n",
    "clf.fit(X_tr,y_tr)\n",
    "y_pre = clf.predict(X_te)\n",
    "print np.mean(y_pre==y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855903297229\n"
     ]
    }
   ],
   "source": [
    "#log\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_tr,y_tr)\n",
    "y_pre = clf.predict(X_te)\n",
    "print np.mean(y_pre==y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Spammer       0.50      0.00      0.01      2408\n",
      "Not spammer       0.86      1.00      0.92     14303\n",
      "\n",
      "avg / total       0.81      0.86      0.79     16711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['Spammer', 'Not spammer']\n",
    "print classification_report(y_te, y_pre, target_names=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dectecting Suspicious Restaurants\n",
    "\n",
    "Now we are able to detect spam reviews and also capture spammers. However, from Yelp’s point of view, even if they can capture spammers and block their account, they still can keep posting spam review with new account. Hence, in order to avoid it, we might want to detect suspicious restaurants and deliver warning to them.\n",
    "Since we are able to detect spams and spammers, we can track the distribution of spam review of a restaurant over time. Here we first make some assumptions. For example, we might assume that if a restaurant has relatively more spam review in early stage, it might be hiring people writing spam review to increase popularity. On the other hand, if a restaurant has unusual frequent spam review, it might be set up by its competitors. \n",
    "\n",
    "Since we are able to detect spams and spammers, we can track the distribution of spam review of a restaurant over time. Here we first make some assumptions. For example, we might assume that if a restaurant has relatively more spam review in early stage, it might be hiring people writing spam review to increase popularity. On the other hand, if a restaurant has unusual frequent spam review, it might be set up by its competitors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Models to other Domains\n",
    "\n",
    "TBD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "TBD"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:PDS]",
   "language": "python",
   "name": "conda-env-PDS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
